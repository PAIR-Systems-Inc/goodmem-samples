{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "intro",
    "description": "Introduction and overview of RAG with GoodMem",
    "language_dependent": false,
    "notebook": "python"
   },
   "source": [
    "# Building a Basic RAG Agent with GoodMem\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial will guide you through building a complete **Retrieval-Augmented Generation (RAG)** system using GoodMem's vector memory capabilities. By the end of this guide, you'll have a functional Q&A system that can:\n",
    "\n",
    "- üîç **Semantically search** through your documents\n",
    "- üìù **Generate contextual answers** using retrieved information \n",
    "- üèóÔ∏è **Scale to handle** large document collections\n",
    "\n",
    "### What is RAG?\n",
    "\n",
    "RAG combines the power of **retrieval** (finding relevant information) with **generation** (creating natural language responses). This approach allows AI systems to provide accurate, context-aware answers by:\n",
    "\n",
    "1. **Retrieving** relevant documents from a knowledge base\n",
    "2. **Augmenting** the query with this context\n",
    "3. **Generating** a comprehensive answer using both the query and retrieved information\n",
    "\n",
    "### Why GoodMem for RAG?\n",
    "\n",
    "GoodMem provides enterprise-grade vector storage with:\n",
    "- **Multiple embedder support** for optimal retrieval accuracy\n",
    "- **Streaming APIs** for real-time responses\n",
    "- **Advanced post-processing** with reranking and summarization\n",
    "- **Scalable architecture** for production workloads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "prerequisites",
    "description": "Prerequisites and requirements",
    "language_dependent": true,
    "notebook": "python"
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before starting, ensure you have:\n",
    "\n",
    "- ‚úÖ **GoodMem server running** locally or access to a remote instance\n",
    "- ‚úÖ **Python 3.9+** installed on your system\n",
    "- ‚úÖ **API key** for your GoodMem instance\n",
    "- ‚úÖ **OpenAI API key** (for embeddings and LLM)\n",
    "- ‚úÖ **Voyage AI API key** (for reranking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "installation-header",
    "description": "Installation and setup section header",
    "language_dependent": false,
    "notebook": "python"
   },
   "source": [
    "## Installation & Setup\n",
    "\n",
    "First, let's install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "installation",
    "description": "Install GoodMem CLI or Python SDK",
    "notebook": "python"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install goodmem-client openai python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "auth-header",
    "description": "Authentication and configuration section header",
    "language_dependent": false,
    "notebook": "python"
   },
   "source": [
    "## Authentication & Configuration\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "GoodMem uses API key authentication to secure your vector memory data. Proper configuration ensures:\n",
    "- **Secure access** to your GoodMem instance\n",
    "- **Isolated environments** (development, staging, production)\n",
    "- **Usage tracking** and access control per API key\n",
    "\n",
    "### What We'll Do\n",
    "\n",
    "1. Configure the GoodMem host URL (where your server is running)\n",
    "2. Set up API key authentication\n",
    "3. Verify the configuration is correct\n",
    "\n",
    "### Configuration Options\n",
    "\n",
    "- **Local development**: `http://localhost:8080` (default)\n",
    "- **Remote/production**: Your deployed GoodMem URL\n",
    "- **Environment variables**: Best practice for managing credentials\n",
    "\n",
    "Let's configure our GoodMem client and test the connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "env-setup",
    "description": "Set up environment variables and API keys",
    "notebook": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GoodMem Host: http://localhost:8080\n",
      "API Key configured: Yes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict, Optional\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables (optional)\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration - Update these values for your setup\n",
    "GOODMEM_HOST = os.getenv('GOODMEM_HOST', 'http://localhost:8080')\n",
    "GOODMEM_API_KEY = os.getenv('GOODMEM_API_KEY', 'your-api-key-here')\n",
    "\n",
    "print(f\"GoodMem Host: {GOODMEM_HOST}\")\n",
    "print(f\"API Key configured: {'Yes' if GOODMEM_API_KEY != 'your-api-key-here' else 'No - Please update'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "transition-to-client-creation",
    "description": "Transition from configuration to client creation",
    "language_dependent": false,
    "notebook": "python"
   },
   "source": [
    "Now let's test the connection to the Goodmem Server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "test-connection",
    "description": "Test connection to GoodMem server",
    "notebook": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully connected to GoodMem!\n",
      "   Found 0 existing spaces\n"
     ]
    }
   ],
   "source": [
    "# Import GoodMem client libraries\n",
    "from goodmem_client.api import SpacesApi, MemoriesApi, EmbeddersApi\n",
    "from goodmem_client.configuration import Configuration\n",
    "from goodmem_client.api_client import ApiClient\n",
    "from goodmem_client.streaming import MemoryStreamClient\n",
    "from goodmem_client.exceptions import ApiException\n",
    "\n",
    "# Configure the API client\n",
    "def create_goodmem_clients():\n",
    "    \"\"\"Create and configure GoodMem API clients.\"\"\"\n",
    "    configuration = Configuration(host=GOODMEM_HOST, \n",
    "                                  api_key={\"ApiKeyAuth\": GOODMEM_API_KEY})\n",
    "    \n",
    "    # Create API client instance\n",
    "    api_client = ApiClient(configuration=configuration)\n",
    "    \n",
    "    # Create API instances\n",
    "    spaces_api = SpacesApi(api_client=api_client)\n",
    "    memories_api = MemoriesApi(api_client=api_client)\n",
    "    embedders_api = EmbeddersApi(api_client=api_client)\n",
    "    stream_client = MemoryStreamClient(api_client)\n",
    "    \n",
    "    return spaces_api, memories_api, embedders_api, stream_client, api_client\n",
    "\n",
    "# Test connection\n",
    "spaces_api, memories_api, embedders_api, stream_client, api_client = create_goodmem_clients()\n",
    "\n",
    "# Test the connection by listing spaces\n",
    "response = spaces_api.list_spaces()\n",
    "print(f\"‚úÖ Successfully connected to GoodMem!\")\n",
    "print(f\"   Found {len(getattr(response, 'spaces', []))} existing spaces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "embedder-header",
    "description": "Embedder explanation and concepts",
    "language_dependent": false,
    "notebook": "python"
   },
   "source": [
    "## Creating an Embedder\n",
    "\n",
    "### Why Embedders Matter\n",
    "\n",
    "An **embedder** is the foundation of semantic search. It converts text into high-dimensional vectors (embeddings) that capture meaning:\n",
    "\n",
    "```\n",
    "Text: \"vacation policy\" ‚Üí Vector: [0.23, -0.45, 0.67, ...]  (1536 dimensions)\n",
    "```\n",
    "\n",
    "These vectors enable:\n",
    "- **Semantic similarity**: Find conceptually similar content, not just keyword matches\n",
    "- **Context understanding**: Capture meaning beyond exact word matches\n",
    "- **Efficient retrieval**: Fast vector comparisons using specialized indexes\n",
    "\n",
    "### The RAG Pipeline Flow\n",
    "\n",
    "```\n",
    "Documents ‚Üí Embedder ‚Üí Vector Storage ‚Üí Semantic Search ‚Üí Retrieved Context\n",
    "```\n",
    "\n",
    "### Choosing an Embedder\n",
    "\n",
    "**OpenAI `text-embedding-3-small`** (what we'll use):\n",
    "- ‚úÖ **High quality**: Excellent for most use cases\n",
    "- ‚úÖ **Fast**: Low latency for real-time applications  \n",
    "- ‚úÖ **1536 dimensions**: Good balance of quality and storage\n",
    "- ‚úÖ **Cost-effective**: $0.02 per 1M tokens\n",
    "\n",
    "**Other options**:\n",
    "- **text-embedding-3-large**: Higher quality, 3072 dimensions, more expensive\n",
    "- **Voyage AI**: Specialized for search, excellent retrieval performance\n",
    "- **Cohere**: Good multilingual support\n",
    "- **Local models**: HuggingFace sentence transformers for privacy/offline\n",
    "\n",
    "### What We'll Do\n",
    "\n",
    "1. Create an OpenAI embedder with proper authentication\n",
    "2. Verify the embedder is ready for use\n",
    "\n",
    "**Note**: You'll need an OpenAI API key set in your environment variable `OPENAI_API_KEY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "create-embedder",
    "description": "Create OpenAI text-embedding-3-small embedder",
    "notebook": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully created OpenAI embedder!\n",
      "   Display Name: OpenAI Text Embedding 3 Small\n",
      "   Embedder ID: 019b3292-6b7e-737c-bb4e-df055b642ea9\n",
      "   Provider: ProviderType.OPENAI\n",
      "   Model: text-embedding-3-small\n",
      "   Dimensionality: 1536\n"
     ]
    }
   ],
   "source": [
    "from goodmem_client.models import EmbedderCreationRequest, ApiKeyAuth, EndpointAuthentication\n",
    "\n",
    "def create_openai_embedder():\n",
    "    \"\"\"Create an OpenAI embedder for text embedding.\"\"\"\n",
    "    \n",
    "    # Check if OPENAI_API_KEY is set\n",
    "    openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "    if not openai_api_key:\n",
    "        print(\"‚ùå OPENAI_API_KEY environment variable not set!\")\n",
    "        return None\n",
    "    \n",
    "    # Create ApiKeyAuth for OpenAI\n",
    "    api_key_auth = ApiKeyAuth(\n",
    "        inline_secret=openai_api_key,\n",
    "        header_name=\"Authorization\",\n",
    "        prefix=\"Bearer \"\n",
    "    )\n",
    "    \n",
    "    # Wrap in EndpointAuthentication\n",
    "    credentials = EndpointAuthentication(\n",
    "        kind=\"CREDENTIAL_KIND_API_KEY\",\n",
    "        api_key=api_key_auth\n",
    "    )\n",
    "    \n",
    "    # Create embedder request with corrected parameters\n",
    "    embedder_request = EmbedderCreationRequest(\n",
    "        display_name=\"OpenAI Text Embedding 3 Small\",\n",
    "        provider_type=\"OPENAI\",\n",
    "        endpoint_url=\"https://api.openai.com/v1\",\n",
    "        model_identifier=\"text-embedding-3-small\",\n",
    "        dimensionality=1536,  # INTEGER, not string\n",
    "        api_path=\"/embeddings\",\n",
    "        distribution_type=\"DENSE\",\n",
    "        supported_modalities=[\"TEXT\"],\n",
    "        credentials=credentials  # EndpointAuthentication object\n",
    "    )\n",
    "    \n",
    "    # Create the embedder\n",
    "    new_embedder = embedders_api.create_embedder(embedder_request)\n",
    "    return new_embedder\n",
    "\n",
    "# Create or retrieve the OpenAI embedder\n",
    "openai_embedder = create_openai_embedder()\n",
    "print(f\"‚úÖ Successfully created OpenAI embedder!\")\n",
    "print(f\"   Display Name: {openai_embedder.display_name}\")\n",
    "print(f\"   Embedder ID: {openai_embedder.embedder_id}\")\n",
    "print(f\"   Provider: {openai_embedder.provider_type}\")\n",
    "print(f\"   Model: {getattr(openai_embedder, 'model_identifier', 'N/A')}\")\n",
    "print(f\"   Dimensionality: {getattr(openai_embedder, 'dimensionality', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "space-header",
    "description": "Space explanation and concepts",
    "language_dependent": false,
    "notebook": "python"
   },
   "source": [
    "## Creating Your First Space\n",
    "\n",
    "### What is a Space?\n",
    "\n",
    "A **Space** in GoodMem is a logical container for organizing related memories (documents). Think of it as a database or collection where you store and retrieve semantically similar content.\n",
    "\n",
    "Each space has:\n",
    "- **Associated embedders**: Which models convert text to vectors\n",
    "- **Chunking configuration**: How documents are split into searchable pieces\n",
    "- **Access controls**: Public or private, with permission management\n",
    "- **Metadata labels**: For organization and filtering\n",
    "\n",
    "### Use Cases for Multiple Spaces\n",
    "\n",
    "You might create different spaces for:\n",
    "- **By domain**: Technical docs, HR policies, product specs\n",
    "- **By environment**: Development, staging, production\n",
    "- **By customer**: Tenant-specific data in multi-tenant apps\n",
    "- **By privacy level**: Public FAQ vs. internal knowledge base\n",
    "\n",
    "### Chunking\n",
    "\n",
    "Documents are too large to search efficiently as whole units. Chunking:\n",
    "- **Improves relevance**: Match specific sections, not entire documents\n",
    "- **Enables context**: Return focused chunks that answer specific questions  \n",
    "- **Optimizes retrieval**: Process and compare smaller text segments\n",
    "\n",
    "**Our chunking strategy**:\n",
    "- **256 characters**: Short enough for focused context, long enough for meaning\n",
    "- **25 character overlap**: Ensures concepts spanning chunk boundaries aren't lost\n",
    "- **Hierarchical separators**: Split on paragraphs first, then sentences, then words\n",
    "\n",
    "### What We'll Do\n",
    "\n",
    "1. List available embedders\n",
    "2. Create a space with our embedder and chunking configuration\n",
    "3. Add metadata labels for organization\n",
    "4. Verify the space is ready\n",
    "\n",
    "Let's create a space for our RAG demo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "list-embedders",
    "description": "List available embedders",
    "notebook": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Available Embedders (1):\n",
      "   1. OpenAI Text Embedding 3 Small - ProviderType.OPENAI\n",
      "      Model: text-embedding-3-small\n",
      "      ID: 019b3292-6b7e-737c-bb4e-df055b642ea9\n",
      "\n",
      "üéØ Using embedder: OpenAI Text Embedding 3 Small\n"
     ]
    }
   ],
   "source": [
    "# First, let's see what embedders are available\n",
    "embedders_response = embedders_api.list_embedders()\n",
    "available_embedders = getattr(embedders_response, 'embedders', [])\n",
    "\n",
    "print(f\"üìã Available Embedders ({len(available_embedders)}):\")\n",
    "for i, embedder in enumerate(available_embedders):\n",
    "    print(f\"   {i+1}. {embedder.display_name} - {embedder.provider_type}\")\n",
    "    print(f\"      Model: {getattr(embedder, 'model_identifier', 'N/A')}\")\n",
    "    print(f\"      ID: {embedder.embedder_id}\")\n",
    "    print()\n",
    "    \n",
    "default_embedder = available_embedders[0]\n",
    "print(f\"üéØ Using embedder: {default_embedder.display_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "transition-to-space-creation",
    "description": "Transition from listing embedders to creating space",
    "language_dependent": false,
    "notebook": "python"
   },
   "source": [
    "Now that we have an embedder configured, let's create a space to store our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_id": "create-space",
    "description": "Create space with chunking configuration",
    "notebook": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created space: RAG Demo Knowledge Base\n",
      "   Space ID: 019b3293-49c8-762d-a4df-799c77c0a5d4\n",
      "   Embedders: 1\n",
      "   Labels: {'purpose': 'rag-demo', 'environment': 'tutorial', 'content-type': 'documentation'}\n"
     ]
    }
   ],
   "source": [
    "from goodmem_client.models import SpaceCreationRequest, SpaceEmbedderConfig\n",
    "\n",
    "# Create a space for our RAG demo\n",
    "SPACE_NAME = \"RAG Demo Knowledge Base\"\n",
    "\n",
    "# Define chunking configuration that we'll reuse throughout the tutorial\n",
    "# Save this configuration to ensure consistency across all memory creation requests\n",
    "DEMO_CHUNKING_CONFIG = {\n",
    "    \"recursive\": {\n",
    "        \"chunk_size\": 256,                     # 256 character chunks for optimal RAG performance\n",
    "        \"chunk_overlap\": 25,                   # 25 character overlap between chunks\n",
    "        \"separators\": [\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],  # Hierarchical splitting\n",
    "        \"keep_strategy\": \"KEEP_END\",           # Append separator to preceding chunk\n",
    "        \"separator_is_regex\": False,           # Plain text separators\n",
    "        \"length_measurement\": \"CHARACTER_COUNT\" # Measure by characters\n",
    "    }\n",
    "}\n",
    "\n",
    "def create_demo_space():\n",
    "    \"\"\"Create a space for our RAG demonstration.\"\"\"\n",
    "    space_embedders = [\n",
    "        SpaceEmbedderConfig(\n",
    "            embedder_id=default_embedder.embedder_id,\n",
    "            default_retrieval_weight=1.0\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Create space request with our saved chunking configuration\n",
    "    create_request = SpaceCreationRequest(\n",
    "        name=SPACE_NAME,\n",
    "        labels={\n",
    "            \"purpose\": \"rag-demo\",\n",
    "            \"environment\": \"tutorial\", \n",
    "            \"content-type\": \"documentation\"\n",
    "        },\n",
    "        space_embedders=space_embedders,\n",
    "        public_read=False,  # Private space\n",
    "        default_chunking_config=DEMO_CHUNKING_CONFIG  # Use our saved config\n",
    "    )\n",
    "    \n",
    "    # Create the space\n",
    "    new_space = spaces_api.create_space(create_request)    \n",
    "    return new_space\n",
    "\n",
    "# Create our demo space\n",
    "demo_space = create_demo_space()\n",
    "print(f\"‚úÖ Created space: {demo_space.name}\")\n",
    "print(f\"   Space ID: {demo_space.space_id}\")\n",
    "print(f\"   Embedders: {len(demo_space.space_embedders)}\")\n",
    "print(f\"   Labels: {dict(demo_space.labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "transition-to-space-verification",
    "description": "Transition from creating space to verifying configuration",
    "language_dependent": false,
    "notebook": "python"
   },
   "source": [
    "Let's verify that our space was created successfully by retrieving its detailed configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_id": "get-space",
    "description": "Get and verify space details",
    "notebook": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Space Configuration:\n",
      "   Name: RAG Demo Knowledge Base\n",
      "   Owner ID: cf5df949-31c6-4c54-af50-f8002107164e\n",
      "   Public Read: False\n",
      "   Created: 1766080072137\n",
      "   Labels: {'purpose': 'rag-demo', 'environment': 'tutorial', 'content-type': 'documentation'}\n",
      "\n",
      "ü§ñ Associated Embedders:\n",
      "   Embedder ID: 019b3292-6b7e-737c-bb4e-df055b642ea9\n",
      "   Retrieval Weight: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Get detailed space information\n",
    "space_details = spaces_api.get_space(demo_space.space_id)\n",
    "\n",
    "print(f\"üîç Space Configuration:\")\n",
    "print(f\"   Name: {space_details.name}\")\n",
    "print(f\"   Owner ID: {space_details.owner_id}\")\n",
    "print(f\"   Public Read: {space_details.public_read}\")\n",
    "print(f\"   Created: {space_details.created_at}\")\n",
    "print(f\"   Labels: {dict(space_details.labels)}\")\n",
    "\n",
    "print(f\"\\nü§ñ Associated Embedders:\")\n",
    "for embedder_assoc in space_details.space_embedders:\n",
    "    print(f\"   Embedder ID: {embedder_assoc.embedder_id}\")\n",
    "    print(f\"   Retrieval Weight: {embedder_assoc.default_retrieval_weight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "documents-header",
    "description": "Document ingestion pipeline explanation",
    "language_dependent": false,
    "notebook": "python"
   },
   "source": [
    "## Adding Documents to Memory\n",
    "\n",
    "### The Document Processing Pipeline\n",
    "\n",
    "When you add a document to GoodMem, it goes through several automated steps:\n",
    "\n",
    "```\n",
    "1. Ingestion ‚Üí 2. Chunking ‚Üí 3. Embedding ‚Üí 4. Indexing ‚Üí 5. Ready for Search\n",
    "```\n",
    "\n",
    "**What happens**:\n",
    "1. **Ingestion**: Document content and metadata are stored\n",
    "2. **Chunking**: Text is split according to your configuration (256 chars, 25 overlap)\n",
    "3. **Embedding**: Each chunk is converted to a vector by your embedder\n",
    "4. **Indexing**: Vectors are indexed for fast similarity search\n",
    "5. **Status**: Document marked as `COMPLETED` and ready for retrieval\n",
    "\n",
    "### Single vs. Batch Operations\n",
    "\n",
    "**Single memory creation** (`CreateMemory`):\n",
    "- ‚úÖ Good for: Real-time ingestion, single documents\n",
    "- ‚úÖ Synchronous processing with immediate status\n",
    "- ‚ö†Ô∏è Higher overhead for bulk operations\n",
    "\n",
    "**Batch memory creation** (`BatchCreateMemory`):\n",
    "- ‚úÖ Good for: Bulk imports, initial setup, periodic updates\n",
    "- ‚úÖ Lower overhead, efficient for multiple documents\n",
    "- ‚úÖ Async processing - check status via `ListMemories`\n",
    "- ‚ö†Ô∏è Takes longer to get individual status feedback\n",
    "\n",
    "### Metadata Best Practices\n",
    "\n",
    "Rich metadata helps with:\n",
    "- **Filtering**: Retrieve specific document types\n",
    "- **Source attribution**: Show users where information came from\n",
    "- **Organization**: Group and manage related documents\n",
    "- **Debugging**: Track ingestion methods and dates\n",
    "\n",
    "### What We'll Do\n",
    "\n",
    "0. [Sample Document Link](https://github.com/PAIR-Systems-Inc/goodmem-samples/tree/main/cookbook/1_Building_a_basic_RAG_Agent_with_GoodMem/sample_documents)\n",
    "1. Load sample documents from local files\n",
    "2. Create one document using single memory creation (to demo the API)\n",
    "3. Create remaining documents using batch operation (more efficient)\n",
    "4. Monitor processing status until all documents are ready\n",
    "\n",
    "We'll use sample company documents that represent common business use cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_id": "load-documents",
    "description": "Load and display sample documents",
    "notebook": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Loaded: company_handbook.txt (2,342 characters)\n",
      "üìÑ Loaded: employee_handbook.pdf (399,615 bytes, base64: 532,820 chars)\n",
      "üìÑ Loaded: product_faq.txt (4,043 characters)\n",
      "üìÑ Loaded: security_policy.txt (4,211 characters)\n",
      "üìÑ Loaded: technical_documentation.txt (2,384 characters)\n",
      "\n",
      "üìö Total documents loaded: 5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import base64\n",
    "\n",
    "# Load our sample documents\n",
    "def load_sample_documents(sample_dir: str) -> List[Dict]:\n",
    "    \"\"\"Load sample documents from the sample_documents directory.\n",
    "    \n",
    "    Automatically discovers all files in the directory and handles:\n",
    "    - .txt files: Read as plain text\n",
    "    - .pdf files: Read as binary and base64 encode\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    # Auto-discover all files in the directory\n",
    "    if not os.path.exists(sample_dir):\n",
    "        print(f\"‚ö†Ô∏è  Directory not found: {sample_dir}\")\n",
    "        return documents\n",
    "    \n",
    "    files = sorted(os.listdir(sample_dir))\n",
    "    \n",
    "    for filename in sorted(files):  # Sort for consistent ordering\n",
    "        filepath = os.path.join(sample_dir, filename)\n",
    "        \n",
    "        # Skip directories\n",
    "        if not os.path.isfile(filepath):\n",
    "            continue\n",
    "        \n",
    "        # Determine file type by extension\n",
    "        file_ext = os.path.splitext(filename)[1].lower()\n",
    "        \n",
    "        if file_ext == '.txt':\n",
    "            # Handle text files\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            documents.append({\n",
    "                'filename': filename,\n",
    "                'content': content,\n",
    "                'content_type': 'text/plain',\n",
    "                'is_binary': False\n",
    "            })\n",
    "            print(f\"üìÑ Loaded: {filename} ({len(content):,} characters)\")\n",
    "        \n",
    "        elif file_ext == '.pdf':\n",
    "            # Handle PDF files\n",
    "            with open(filepath, 'rb') as f:\n",
    "                binary_content = f.read()\n",
    "            \n",
    "            # Base64 encode the binary content\n",
    "            content_b64 = base64.b64encode(binary_content).decode('utf-8')\n",
    "            \n",
    "            documents.append({\n",
    "                'filename': filename,\n",
    "                'content_b64': content_b64,\n",
    "                'content_type': 'application/pdf',\n",
    "                'is_binary': True\n",
    "            })\n",
    "            print(f\"üìÑ Loaded: {filename} ({len(binary_content):,} bytes, base64: {len(content_b64):,} chars)\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Skipping unsupported file type: {filename}\")\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Load the documents\n",
    "sample_docs = load_sample_documents(\"sample_documents\")\n",
    "print(f\"\\nüìö Total documents loaded: {len(sample_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "transition-to-memory-creation",
    "description": "Transition from loading documents to creating memories",
    "language_dependent": false,
    "notebook": "python"
   },
   "source": [
    "Now that we have documents loaded, let's create memories from them. We'll start by creating one memory individually to demonstrate the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "create-single-memory",
    "description": "Create single memory for demonstration",
    "notebook": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Creating first document using CreateMemory API:\n",
      "   Document: company_handbook.txt\n",
      "   Content Type: text/plain\n",
      "   Method: Individual memory creation\n"
     ]
    }
   ],
   "source": [
    "# Create the first memory individually to demonstrate single memory creation\n",
    "from goodmem_client.models import MemoryCreationRequest\n",
    "\n",
    "# Function to create a memory request\n",
    "def build_memory_request(space_id: str, document: dict) -> MemoryCreationRequest:\n",
    "    \"\"\"Create a MemoryCreationRequest based on the document type.\"\"\"\n",
    "    if document.get('is_binary', False):\n",
    "        # For binary files (PDF), use original_content_b64\n",
    "        memory_request = MemoryCreationRequest(\n",
    "            space_id=space_id,\n",
    "            original_content_b64=document['content_b64'],  # Base64 encoded\n",
    "            content_type=document['content_type'],          # application/pdf\n",
    "            metadata={\n",
    "                \"filename\": document['filename'],\n",
    "                \"source\": \"sample_documents\",\n",
    "            },\n",
    "            chunkingConfig=DEMO_CHUNKING_CONFIG\n",
    "        )\n",
    "    else:\n",
    "        # For text files, use original_content\n",
    "        memory_request = MemoryCreationRequest(\n",
    "            space_id=space_id,\n",
    "            original_content=document['content'],          # Plain text\n",
    "            content_type=document['content_type'],         # text/plain\n",
    "            metadata={\n",
    "                \"filename\": document['filename'],\n",
    "                \"source\": \"sample_documents\",\n",
    "            },\n",
    "            chunkingConfig=DEMO_CHUNKING_CONFIG\n",
    "        )\n",
    "    return memory_request\n",
    "\n",
    "first_doc = sample_docs[0]\n",
    "single_memory = memories_api.create_memory(\n",
    "    build_memory_request(demo_space.space_id, first_doc)\n",
    ")\n",
    "print(f\"üìù Creating first document using CreateMemory API:\")\n",
    "print(f\"   Document: {first_doc['filename']}\")\n",
    "print(f\"   Content Type: {first_doc['content_type']}\")\n",
    "print(f\"   Method: Individual memory creation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "transition-to-memory-verification",
    "description": "Transition from creating memory to verifying it",
    "language_dependent": false,
    "notebook": "python"
   },
   "source": [
    "Let's verify the memory was created successfully by retrieving it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_id": "get-memory-with-content",
    "description": "Retrieve memory with content using Python SDK",
    "notebook": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Successfully retrieved memory:\n",
      "   Memory ID: 019b3295-5a9d-703d-9954-43e6153ff9c9\n",
      "   Space ID: 019b3293-49c8-762d-a4df-799c77c0a5d4\n",
      "   Status: COMPLETED\n",
      "   Content Type: text/plain\n",
      "   Created At: 1766080207518\n",
      "   Updated At: 1766080211751\n",
      "\n",
      "   üìã Metadata:\n",
      "      source: sample_documents\n",
      "      filename: company_handbook.txt\n",
      "      ingestion_method: single\n",
      "\n",
      "‚úÖ Content retrieved and decoded:\n",
      "   Content length: 2342 characters\n",
      "   First 200 chars: ACME Corporation Employee Handbook\n",
      "\n",
      "Welcome to ACME Corporation! This handbook provides essential information about our company policies, procedures, and culture.\n",
      "\n",
      "COMPANY OVERVIEW\n",
      "ACME Corporation is...\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate retrieving a memory by ID using get_memory\n",
    "retrieved_memory = memories_api.get_memory(\n",
    "    id=single_memory.memory_id,\n",
    "    include_content=True\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Successfully retrieved memory:\")\n",
    "print(f\"   Memory ID: {retrieved_memory.memory_id}\")\n",
    "print(f\"   Space ID: {retrieved_memory.space_id}\")\n",
    "print(f\"   Status: {retrieved_memory.processing_status}\")\n",
    "print(f\"   Content Type: {retrieved_memory.content_type}\")\n",
    "print(f\"   Created At: {retrieved_memory.created_at}\")\n",
    "print(f\"   Updated At: {retrieved_memory.updated_at}\")\n",
    "\n",
    "if retrieved_memory.metadata:\n",
    "    print(f\"\\n   üìã Metadata:\")\n",
    "    for key, value in retrieved_memory.metadata.items():\n",
    "        print(f\"      {key}: {value}\")\n",
    "\n",
    "if retrieved_memory.original_content:\n",
    "    # Decode the base64 encoded content\n",
    "    decoded_content = base64.b64decode(retrieved_memory.original_content).decode('utf-8')\n",
    "    print(f\"\\n‚úÖ Content retrieved and decoded:\")\n",
    "    print(f\"   Content length: {len(decoded_content)} characters\")\n",
    "    print(f\"   First 200 chars: {decoded_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "transition-to-batch-creation",
    "description": "Transition from single memory to batch creation",
    "language_dependent": false,
    "notebook": "python"
   },
   "source": [
    "For the remaining documents, we'll use batch creation which is more efficient for multiple documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cell_id": "batch-create-memories",
    "description": "Batch create memories from directory",
    "notebook": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Creating 4 memories using BatchCreateMemory API:\n",
      "\n",
      "üìã Total Memory Creation Summary:\n",
      "   üìÑ Single CreateMemory: 1 document\n",
      "   üì¶ Batch CreateMemory: 4 documents submitted\n",
      "   ‚è≥ Check processing status in the next cell\n"
     ]
    }
   ],
   "source": [
    "# Create the remaining documents using batch memory creation\n",
    "from goodmem_client.models import BatchMemoryCreationRequest\n",
    "\n",
    "def create_batch_memories(space_id: str, documents: List[dict]) -> List[dict]:\n",
    "    \"\"\"Create multiple memories in GoodMem using batch creation for efficiency.\"\"\"\n",
    "    \n",
    "    # Prepare batch memory requests using our saved chunking configuration\n",
    "    memory_requests = []\n",
    "    for i, doc in enumerate(documents):\n",
    "        memory_requests.append(build_memory_request(space_id, doc))\n",
    "    \n",
    "    # Create batch request\n",
    "    batch_request = BatchMemoryCreationRequest(\n",
    "        requests=memory_requests\n",
    "    )\n",
    "    \n",
    "    print(f\"üì¶ Creating {len(memory_requests)} memories using BatchCreateMemory API:\")\n",
    "    # Execute batch creation - this returns None on success\n",
    "    results = memories_api.batch_create_memory(batch_request).results\n",
    "    return [item.memory for item in results]\n",
    "\n",
    "# Create the remaining documents (skip the first one we already created)\n",
    "remaining_docs = sample_docs[1:]  # All documents except the first\n",
    "created_memories = create_batch_memories(demo_space.space_id, remaining_docs)\n",
    "\n",
    "print(f\"\\nüìã Total Memory Creation Summary:\")\n",
    "print(f\"   üìÑ Single CreateMemory: 1 document\")\n",
    "print(f\"   üì¶ Batch CreateMemory: {len(remaining_docs)} documents submitted\")\n",
    "print(f\"   ‚è≥ Check processing status in the next cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "transition-to-batch-retrieval",
    "description": "Transition from batch creation to checking status",
    "language_dependent": false,
    "notebook": "python"
   },
   "source": [
    "Let's check the status of all the memories we created to see if they're ready for searching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cell_id": "list-memories",
    "description": "List all memories in space",
    "notebook": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1. company_handbook.txt\n",
      "      Status: COMPLETED\n",
      "      Description: No description\n",
      "      Created: 1766080207518\n",
      "\n",
      "   2. employee_handbook.pdf\n",
      "      Status: COMPLETED\n",
      "      Description: No description\n",
      "      Created: 1766080248638\n",
      "\n",
      "   3. product_faq.txt\n",
      "      Status: COMPLETED\n",
      "      Description: No description\n",
      "      Created: 1766080248638\n",
      "\n",
      "   4. security_policy.txt\n",
      "      Status: COMPLETED\n",
      "      Description: No description\n",
      "      Created: 1766080248638\n",
      "\n",
      "   5. technical_documentation.txt\n",
      "      Status: COMPLETED\n",
      "      Description: No description\n",
      "      Created: 1766080248638\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get all memories we just ingested to verify they're ready\n",
    "from goodmem_client.models import BatchMemoryRetrievalRequest\n",
    "\n",
    "def batch_retrieve_memories(memory_ids: List[str]) -> List[dict]:\n",
    "    \"\"\"Retrieve multiple memories by their IDs using batch retrieval.\"\"\"\n",
    "    batch_request = BatchMemoryRetrievalRequest(\n",
    "        memory_ids=memory_ids,\n",
    "        include_content=False  # We don't need content for status check\n",
    "    )\n",
    "    results = memories_api.batch_get_memory(batch_request).results\n",
    "    return [item.memory for item in results]\n",
    "\n",
    "memory_ids = [single_memory.memory_id] + [mem.memory_id for mem in created_memories]\n",
    "memories = batch_retrieve_memories(memory_ids)\n",
    "for i, memory in enumerate(memories, 1):\n",
    "    metadata = memory.metadata or {}\n",
    "    filename = metadata.get('filename', 'Unknown')\n",
    "    description = metadata.get('description', 'No description')\n",
    "    \n",
    "    print(f\"   {i}. {filename}\")\n",
    "    print(f\"      Status: {memory.processing_status}\")\n",
    "    print(f\"      Description: {description}\")\n",
    "    print(f\"      Created: {memory.created_at}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "transition-to-processing-monitor",
    "description": "Transition from batch retrieval to monitoring processing",
    "language_dependent": false,
    "notebook": "python"
   },
   "source": [
    "Since batch memories are processed asynchronously, let's monitor their processing status until they're all ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cell_id": "monitor-processing",
    "description": "Monitor memory processing status",
    "notebook": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Waiting for document processing to complete...\n",
      "üí° Note: Batch memories are processed asynchronously\n",
      "\n",
      "üìä Processing status: {'COMPLETED': 5} (Total: 5 memories)\n",
      "‚úÖ All documents processed successfully!\n",
      "üéâ Ready for semantic search and retrieval!\n"
     ]
    }
   ],
   "source": [
    "# Monitor processing status for all created memories\n",
    "def wait_for_processing_completion(memory_ids: List[str], max_wait_seconds: int = 120):\n",
    "    \"\"\"Wait for memories to finish processing.\"\"\"\n",
    "    start_time = time.time()\n",
    "    while time.time() - start_time < max_wait_seconds:\n",
    "        # List memories in our space\n",
    "        memories = batch_retrieve_memories(memory_ids)\n",
    "        \n",
    "        # Check processing status\n",
    "        status_counts = {}\n",
    "        for memory in memories:\n",
    "            status = memory.processing_status\n",
    "            status_counts[status] = status_counts.get(status, 0) + 1\n",
    "        \n",
    "        print(f\"üìä Processing status: {dict(status_counts)} (Total: {len(memories)} memories)\")\n",
    "        \n",
    "        # Check if all are completed\n",
    "        if all(memory.processing_status == 'COMPLETED' for memory in memories):\n",
    "            print(\"‚úÖ All documents processed successfully!\")\n",
    "            return True\n",
    "            \n",
    "        # Check for any failures\n",
    "        failed_count = status_counts.get('FAILED', 0)\n",
    "        if failed_count > 0:\n",
    "            print(f\"‚ùå {failed_count} memories failed processing\")\n",
    "            return False\n",
    "        \n",
    "        time.sleep(5)  # Wait 5 seconds before checking again\n",
    "    \n",
    "    print(f\"‚è∞ Timeout waiting for processing (waited {max_wait_seconds}s)\")\n",
    "    return False\n",
    "\n",
    "# Wait for processing to complete for all memories (single + batch)\n",
    "# Since batch_create_memory returns None, we monitor by listing all memories\n",
    "print(\"‚è≥ Waiting for document processing to complete...\")\n",
    "print(\"üí° Note: Batch memories are processed asynchronously\\n\")\n",
    "processing_complete = wait_for_processing_completion(memory_ids)\n",
    "\n",
    "if processing_complete:\n",
    "    print(\"üéâ Ready for semantic search and retrieval!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Some documents may still be processing. You can continue with the tutorial.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "search-header",
    "description": "Semantic search explanation and concepts",
    "language_dependent": false,
    "notebook": "python"
   },
   "source": [
    "## Semantic Search & Retrieval\n",
    "\n",
    "### Why Semantic Search?\n",
    "\n",
    "**Traditional keyword search**:\n",
    "- Matches exact words or simple variations\n",
    "- Misses conceptually similar content with different wording\n",
    "- Example: \"vacation days\" won't match \"time off policy\"\n",
    "\n",
    "**Semantic search**:\n",
    "- Understands meaning and context\n",
    "- Finds conceptually similar content regardless of exact wording\n",
    "- Example: \"vacation days\" successfully matches \"time off policy\"\n",
    "\n",
    "### How It Works\n",
    "\n",
    "```\n",
    "Query: \"vacation policy\" \n",
    "   ‚Üì (embed with same embedder)\n",
    "Query Vector: [0.23, -0.45, ...]\n",
    "   ‚Üì (compare to all chunk vectors)\n",
    "Most Similar Chunks: (by cosine similarity)\n",
    "   1. \"TIME OFF POLICY...\" (score: -0.604)\n",
    "   2. \"Vacation requests...\" (score: -0.544)\n",
    "   3. \"WORK HOURS...\" (score: -0.458)\n",
    "```\n",
    "\n",
    "### Understanding Relevance Scores\n",
    "\n",
    "GoodMem uses **cosine distance** (negative cosine similarity):\n",
    "- **Lower values = more relevant** (e.g., -0.6 is better than -0.4)\n",
    "- **Range**: Typically -1.0 (most similar) to 0.0 (unrelated)\n",
    "- **Good threshold**: Results under -0.3 are usually relevant\n",
    "- **Context matters**: Exact scores vary by embedder and content\n",
    "\n",
    "### Streaming API Benefits\n",
    "\n",
    "GoodMem's streaming API:\n",
    "- **Real-time results**: Process chunks as they arrive\n",
    "- **Low latency**: Start showing results immediately\n",
    "- **Memory efficient**: No need to buffer entire result set\n",
    "- **Progressive UI**: Update interface as more results come in\n",
    "\n",
    "### What We'll Do\n",
    "\n",
    "1. Implement a semantic search function using GoodMem's streaming API\n",
    "2. Process different event types (chunks, memories, metadata)\n",
    "3. Display results with relevance scores\n",
    "4. Test with various queries to see semantic matching in action\n",
    "\n",
    "Now comes the exciting part! Let's perform semantic search using GoodMem's streaming API. This will:\n",
    "\n",
    "- **Find relevant chunks** based on semantic similarity\n",
    "- **Stream results** in real-time\n",
    "- **Include relevance scores** for ranking\n",
    "- **Return structured data** for easy processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cell_id": "semantic-search",
    "description": "Define semantic search function",
    "notebook": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Searching for: 'What is the vacation policy for employees?'\n",
      "üìÅ Space ID: 019b3293-49c8-762d-a4df-799c77c0a5d4\n",
      "üìä Max results: 5\n",
      "--------------------------------------------------\n",
      "üìÑ Chunk 1:\n",
      "   Relevance: -0.680\n",
      "   Text: TIME OFF POLICY\n",
      "All full-time employees receive:\n",
      "- 15 days of paid vacation annually (increases to 20 days after 3 years)\n",
      "- 10 sick days per year\n",
      "- 8 company holidays\n",
      "- Personal days as needed with manager approval...\n",
      "\n",
      "üìÑ Chunk 2:\n",
      "   Relevance: -0.675\n",
      "   Text: 1.  Eligibility \n",
      "\n",
      " \n",
      "All regular full-time employees are eligible for vacation benefits. \n",
      "\n",
      " \n",
      "2.  Accrual \n",
      "\n",
      " \n",
      "Eligible employees accrue vacation in accordance with the following scheduleix: \n",
      "\n",
      " \n",
      "Years of Continuous Service Rate of Accrual \n",
      "\n",
      "Date of hire through end of year \n",
      "5...\n",
      "\n",
      "üìÑ Chunk 3:\n",
      "   Relevance: -0.662\n",
      "   Text: [ORGANIZATION] has established the following vacation plan to provide eligible employees \n",
      "time off with pay so that they may be free from their regular duties for a period of rest and \n",
      "relaxation without loss of pay or benefits. \n",
      "\n",
      " \n",
      "1.  Eligibility...\n",
      "\n",
      "üìÑ Chunk 4:\n",
      "   Relevance: -0.646\n",
      "   Text: Vacation Pay: Vacation pay shall be based on the employee‚Äôs regular base rate and \n",
      "working schedule, exclusive of overtime.  No employee will receive pay in lieu of \n",
      "vacation, except on termination of employment, as discussed below....\n",
      "\n",
      "üìÑ Chunk 5:\n",
      "   Relevance: -0.643\n",
      "   Text: employees can use paid vacation time in minimum increments of one day.xii \n",
      "\n",
      " \n",
      "Accumulating Vacation: Employees are encouraged to use available paid vacation time \n",
      "for rest and relaxation.  In the event that accrued vacation is not used by the end of the...\n",
      "\n",
      "‚úÖ Search completed: 5 chunks found, 9 events processed\n"
     ]
    }
   ],
   "source": [
    "def semantic_search(query: str, space_id: str, max_results: int = 5) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Perform semantic search using GoodMem's streaming API.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query\n",
    "        space_id: ID of the space to search\n",
    "        max_results: Maximum number of results to return\n",
    "    \n",
    "    Returns:\n",
    "        List of search results with chunks and metadata\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"üîç Searching for: '{query}'\")\n",
    "    print(f\"üìÅ Space ID: {space_id}\")\n",
    "    print(f\"üìä Max results: {max_results}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Perform streaming search\n",
    "    event_count = 0\n",
    "    retrieved_chunks = []\n",
    "    \n",
    "    for event in stream_client.retrieve_memory_stream(\n",
    "        message=query,\n",
    "        space_ids=[space_id],\n",
    "        requested_size=max_results,\n",
    "        fetch_memory=True,\n",
    "        fetch_memory_content=False,  # We don't need full content for this demo\n",
    "        format=\"ndjson\"\n",
    "    ):\n",
    "        event_count += 1\n",
    "        \n",
    "        if event.retrieved_item and event.retrieved_item.chunk:\n",
    "            chunk_info = event.retrieved_item.chunk\n",
    "            chunk_data = chunk_info.chunk\n",
    "            \n",
    "            retrieved_chunks.append({\n",
    "                'chunk_text': chunk_data.get('chunkText', ''),\n",
    "                'relevance_score': chunk_info.relevance_score,\n",
    "                'memory_index': chunk_info.memory_index,\n",
    "                'result_set_id': chunk_info.result_set_id,\n",
    "                'chunk_sequence': chunk_data.get('chunkSequenceNumber', 0)\n",
    "            })\n",
    "            \n",
    "            print(f\"üìÑ Chunk {len(retrieved_chunks)}:\")\n",
    "            print(f\"   Relevance: {chunk_info.relevance_score:.3f}\")\n",
    "            print(f\"   Text: {chunk_data.get('chunkText', '')}...\")\n",
    "            print()\n",
    "    \n",
    "    print(f\"‚úÖ Search completed: {len(retrieved_chunks)} chunks found, {event_count} events processed\")\n",
    "    return retrieved_chunks\n",
    "\n",
    "# Test semantic search with a sample query\n",
    "sample_query = \"What is the vacation policy for employees?\"\n",
    "search_results = semantic_search(sample_query, demo_space.space_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "transition-to-search-testing",
    "description": "Transition from search function to testing queries",
    "language_dependent": false,
    "notebook": "python"
   },
   "source": [
    "Let's test our semantic search function with various queries to see how it finds relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cell_id": "test-queries",
    "description": "Test multiple semantic search queries",
    "notebook": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Test Query 1: How do I reset my password?\n",
      "============================================================\n",
      "üîç Searching for: 'How do I reset my password?'\n",
      "üìÅ Space ID: 019b3293-49c8-762d-a4df-799c77c0a5d4\n",
      "üìä Max results: 3\n",
      "--------------------------------------------------\n",
      "üìÑ Chunk 1:\n",
      "   Relevance: -0.370\n",
      "   Text: password they use to gain access to computers or the Internet, as well as any change to \n",
      "such password.  Such notice must be made immediately. \n",
      "\n",
      " \n",
      "4. Compliance...\n",
      "\n",
      "üìÑ Chunk 2:\n",
      "   Relevance: -0.363\n",
      "   Text: - No reuse of last 12 passwords\n",
      "- Must be changed every 90 days for privileged accounts\n",
      "- Multi-factor authentication required for all business systems\n",
      "- Password managers recommended for personal password storage\n",
      "\n",
      "ACCEPTABLE USE POLICY...\n",
      "\n",
      "üìÑ Chunk 3:\n",
      "   Relevance: -0.306\n",
      "   Text: Each classification level has specific handling, storage, and transmission requirements outlined in our data handling procedures.\n",
      "\n",
      "PASSWORD POLICY\n",
      "Strong passwords are essential for system security:\n",
      "- Minimum 12 characters with mix of letters, numbers, and symbols...\n",
      "\n",
      "‚úÖ Search completed: 3 chunks found, 7 events processed\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "üîç Test Query 2: What are the security requirements for remote work?\n",
      "============================================================\n",
      "üîç Searching for: 'What are the security requirements for remote work?'\n",
      "üìÅ Space ID: 019b3293-49c8-762d-a4df-799c77c0a5d4\n",
      "üìä Max results: 3\n",
      "--------------------------------------------------\n",
      "üìÑ Chunk 1:\n",
      "   Relevance: -0.710\n",
      "   Text: - Report suspicious emails or security incidents immediately\n",
      "\n",
      "REMOTE WORK SECURITY\n",
      "Remote employees must follow additional security measures:\n",
      "- Use company-approved VPN for all work connections\n",
      "- Ensure home WiFi networks use WPA3 encryption...\n",
      "\n",
      "üìÑ Chunk 2:\n",
      "   Relevance: -0.530\n",
      "   Text: - Keep work devices physically secure and locked when unattended\n",
      "- Use only approved cloud storage services for company data\n",
      "- Install automatic security updates on all devices\n",
      "\n",
      "INCIDENT RESPONSE\n",
      "Security incidents must be reported immediately:...\n",
      "\n",
      "üìÑ Chunk 3:\n",
      "   Relevance: -0.471\n",
      "   Text: SECURITY TRAINING\n",
      "All employees must complete:\n",
      "- Security awareness training within 30 days of hire\n",
      "- Annual security refresher training\n",
      "- Role-specific security training as required\n",
      "- Phishing simulation exercises quarterly...\n",
      "\n",
      "‚úÖ Search completed: 3 chunks found, 6 events processed\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "üîç Test Query 3: API authentication and rate limits\n",
      "============================================================\n",
      "üîç Searching for: 'API authentication and rate limits'\n",
      "üìÅ Space ID: 019b3293-49c8-762d-a4df-799c77c0a5d4\n",
      "üìä Max results: 3\n",
      "--------------------------------------------------\n",
      "üìÑ Chunk 1:\n",
      "   Relevance: -0.554\n",
      "   Text: BASE URL\n",
      "All API endpoints are accessed via:\n",
      "https://api.acme.com/v1/\n",
      "\n",
      "RATE LIMITING\n",
      "API requests are limited to:\n",
      "- 1000 requests per hour for free accounts\n",
      "- 10000 requests per hour for premium accounts\n",
      "- Rate limit headers are included in all responses\n",
      "\n",
      "CORE ENDPOINTS...\n",
      "\n",
      "üìÑ Chunk 2:\n",
      "   Relevance: -0.462\n",
      "   Text: - 403: Forbidden - Insufficient permissions\n",
      "- 404: Not Found - Resource does not exist\n",
      "- 429: Too Many Requests - Rate limit exceeded\n",
      "- 500: Internal Server Error...\n",
      "\n",
      "üìÑ Chunk 3:\n",
      "   Relevance: -0.458\n",
      "   Text: AUTHENTICATION\n",
      "All API requests require authentication using API keys. Include your API key in the request header:\n",
      "\n",
      "Authorization: Bearer YOUR_API_KEY\n",
      "\n",
      "API keys can be generated from your account dashboard. Keep your API keys secure and do not share them publicly....\n",
      "\n",
      "‚úÖ Search completed: 3 chunks found, 6 events processed\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "üîç Test Query 4: Employee benefits and health insurance\n",
      "============================================================\n",
      "üîç Searching for: 'Employee benefits and health insurance'\n",
      "üìÅ Space ID: 019b3293-49c8-762d-a4df-799c77c0a5d4\n",
      "üìä Max results: 3\n",
      "--------------------------------------------------\n",
      "üìÑ Chunk 1:\n",
      "   Relevance: -0.600\n",
      "   Text: - Health insurance with 90% premium coverage\n",
      "- Dental and vision insurance\n",
      "- 401(k) retirement plan with 4% company match\n",
      "- Life insurance equal to 2x annual salary\n",
      "- Employee stock options after 1 year of employment\n",
      "- Professional development allowance of $2,000 annually...\n",
      "\n",
      "üìÑ Chunk 2:\n",
      "   Relevance: -0.590\n",
      "   Text: A. Employee Benefits.......................................................................................................................13...\n",
      "\n",
      "üìÑ Chunk 3:\n",
      "   Relevance: -0.586\n",
      "   Text: [ORGANIZATION]. \n",
      "\n",
      " \n",
      "F. Health Insurance \n",
      "\n",
      " \n",
      "\n",
      "All employees classified by [ORGANIZATION] as regularly working at least 30 hours per week \n",
      "and their dependents currently are eligible to participate in [ORGANIZATION]‚Äôs medical,...\n",
      "\n",
      "‚úÖ Search completed: 3 chunks found, 7 events processed\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "üîç Test Query 5: How much does the software cost?\n",
      "============================================================\n",
      "üîç Searching for: 'How much does the software cost?'\n",
      "üìÅ Space ID: 019b3293-49c8-762d-a4df-799c77c0a5d4\n",
      "üìä Max results: 3\n",
      "--------------------------------------------------\n",
      "üìÑ Chunk 1:\n",
      "   Relevance: -0.496\n",
      "   Text: A: The ACME Software Suite is an integrated platform that provides business management tools including CRM, project management, analytics, and automation capabilities. It's designed to streamline operations for small to medium-sized businesses.\n",
      "\n",
      "Q: How much does ACME cost?...\n",
      "\n",
      "üìÑ Chunk 2:\n",
      "   Relevance: -0.471\n",
      "   Text: A: We offer three pricing tiers:\n",
      "- Starter: $29/month for up to 5 users\n",
      "- Professional: $79/month for up to 25 users\n",
      "- Enterprise: $199/month for unlimited users\n",
      "All plans include core features with increasing storage and advanced functionality....\n",
      "\n",
      "üìÑ Chunk 3:\n",
      "   Relevance: -0.425\n",
      "   Text: Q: Is there a free trial available?\n",
      "A: Yes, we offer a 14-day free trial with full access to all Professional tier features. No credit card required to start your trial.\n",
      "\n",
      "TECHNICAL QUESTIONS...\n",
      "\n",
      "‚úÖ Search completed: 3 chunks found, 6 events processed\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Let's try a few different queries to see how semantic search works\n",
    "def test_multiple_queries(space_id: str):\n",
    "    \"\"\"Test semantic search with different types of queries.\"\"\"\n",
    "    \n",
    "    test_queries = [\n",
    "        \"How do I reset my password?\",\n",
    "        \"What are the security requirements for remote work?\", \n",
    "        \"API authentication and rate limits\",\n",
    "        \"Employee benefits and health insurance\",\n",
    "        \"How much does the software cost?\"\n",
    "    ]\n",
    "    \n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        print(f\"\\nüîç Test Query {i}: {query}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        semantic_search(query, space_id, max_results=3)\n",
    "        \n",
    "        print(\"\\n\" + \"-\" * 60)\n",
    "\n",
    "test_multiple_queries(demo_space.space_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "troubleshooting-search",
    "description": "Troubleshooting guide for search issues",
    "notebook": "python"
   },
   "source": [
    "## Troubleshooting Search Results\n",
    "\n",
    "**Empty or weak results?** Try these:\n",
    "\n",
    "- **Increase `max_results` or `maxResults`** ‚Üí More candidates to find relevant matches\n",
    "- **Adjust chunking** ‚Üí Larger chunks (512) for context, smaller (128) for precision  \n",
    "- **Check embedder** ‚Üí Verify API credentials and model configuration\n",
    "- **Verify processing** ‚Üí Ensure all memories show `COMPLETED` status\n",
    "- **Refine query** ‚Üí Be more specific with natural language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "advanced-header",
    "description": "Advanced features section header",
    "language_dependent": false,
    "notebook": "python"
   },
   "source": [
    "## Advanced Features\n",
    "\n",
    "Congratulations! üéâ You've successfully built a semantic search system using GoodMem. Here's what you've accomplished:\n",
    "\n",
    "### ‚úÖ What You Built\n",
    "- **Document ingestion pipeline** with automatic chunking and embedding\n",
    "- **Semantic search system** with relevance scoring\n",
    "- **Simple Q&A system** using GoodMem's vector capabilities\n",
    "\n",
    "### üöÄ Next Steps for Advanced Implementation\n",
    "\n",
    "#### Reranking\n",
    "Improve search quality by adding a reranking stage. **Rerankers** are specialized models that re-score search results to improve relevance:\n",
    "\n",
    "- **Two-stage retrieval**: Fast initial retrieval with embeddings, then precise reranking\n",
    "- **Better relevance**: Rerankers use cross-attention to understand query-document relationships\n",
    "- **Reduced costs**: Rerank only top-K results instead of entire corpus\n",
    "- **Voyage AI reranker**: Industry-leading reranking model with state-of-the-art performance\n",
    "\n",
    "The combination of fast embedding-based retrieval followed by accurate reranking provides the best balance of speed and quality for production RAG systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "reranker-header",
    "description": "Reranker explanation and concepts",
    "language_dependent": false,
    "notebook": "python"
   },
   "source": [
    "## Configuring a Reranker\n",
    "\n",
    "To further improve search quality, we can add a **reranker** to our RAG pipeline. While embedders provide fast semantic search, rerankers use more sophisticated models to re-score the top results for better accuracy.\n",
    "\n",
    "### Why Use Reranking?\n",
    "\n",
    "1. **Higher Accuracy**: Rerankers use cross-encoder architectures that directly compare queries and documents\n",
    "2. **Two-Stage Pipeline**: Fast retrieval with embeddings + precise reranking = optimal performance\n",
    "3. **Cost Effective**: Only rerank top-K results (e.g., top 20) rather than entire corpus\n",
    "\n",
    "### Voyage AI Reranker\n",
    "\n",
    "We'll use Voyage AI's `rerank-2.5` model, which provides:\n",
    "- **State-of-the-art performance** on reranking benchmarks\n",
    "- **Fast inference** optimized for production use\n",
    "- **Simple API** that integrates seamlessly with GoodMem\n",
    "\n",
    "**Note**: You'll need a Voyage AI API key set in your environment variable `VOYAGE_API_KEY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "create-reranker",
    "description": "Create Voyage AI reranker",
    "notebook": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully created Voyage reranker!\n",
      "   Display Name: Voyage Rerank 2.5\n",
      "   Reranker ID: 019b3297-8264-7418-aafc-3fb7ce17a64e\n",
      "   Provider: ProviderType.VOYAGE\n",
      "   Model: rerank-2.5\n"
     ]
    }
   ],
   "source": [
    "from goodmem_client.api import RerankersApi\n",
    "from goodmem_client.models import RerankerCreationRequest, ApiKeyAuth, EndpointAuthentication\n",
    "\n",
    "def create_voyage_reranker():\n",
    "    \"\"\"Create a Voyage AI reranker for improving search results.\"\"\"\n",
    "    \n",
    "    # Check if VOYAGE_API_KEY is set\n",
    "    voyage_api_key = os.getenv('VOYAGE_API_KEY')\n",
    "    if not voyage_api_key:\n",
    "        print(\"‚ùå VOYAGE_API_KEY environment variable not set!\")\n",
    "        return None\n",
    "    \n",
    "    # Create RerankersApi instance\n",
    "    rerankers_api = RerankersApi(api_client=api_client)\n",
    "    \n",
    "    # Create ApiKeyAuth for Voyage\n",
    "    api_key_auth = ApiKeyAuth(\n",
    "        inline_secret=voyage_api_key,\n",
    "        header_name=\"Authorization\",\n",
    "        prefix=\"Bearer \"\n",
    "    )\n",
    "    \n",
    "    # Wrap in EndpointAuthentication\n",
    "    credentials = EndpointAuthentication(\n",
    "        kind=\"CREDENTIAL_KIND_API_KEY\",\n",
    "        api_key=api_key_auth\n",
    "    )\n",
    "    \n",
    "    # Create reranker request\n",
    "    reranker_request = RerankerCreationRequest(\n",
    "        display_name=\"Voyage Rerank 2.5\",\n",
    "        provider_type=\"VOYAGE\",\n",
    "        endpoint_url=\"https://api.voyageai.com\",\n",
    "        model_identifier=\"rerank-2.5\",\n",
    "        api_path=\"/v1/rerank\",\n",
    "        supported_modalities=[\"TEXT\"],\n",
    "        credentials=credentials,\n",
    "        description=\"Voyage AI reranker for improving search result relevance\"\n",
    "    )\n",
    "    \n",
    "    # Create the reranker\n",
    "    new_reranker = rerankers_api.create_reranker(reranker_request)    \n",
    "    return new_reranker\n",
    "\n",
    "# Create or retrieve the Voyage reranker\n",
    "voyage_reranker = create_voyage_reranker()\n",
    "print(f\"‚úÖ Successfully created Voyage reranker!\")\n",
    "print(f\"   Display Name: {voyage_reranker.display_name}\")\n",
    "print(f\"   Reranker ID: {voyage_reranker.reranker_id}\")\n",
    "print(f\"   Provider: {voyage_reranker.provider_type}\")\n",
    "print(f\"   Model: {getattr(voyage_reranker, 'model_identifier', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "llm-header",
    "description": "LLM explanation and role in RAG",
    "language_dependent": false,
    "notebook": "python"
   },
   "source": [
    "## Registering an LLM\n",
    "\n",
    "The final component in our RAG pipeline is the **LLM (Large Language Model)** - the generation component that creates natural language responses using the retrieved and reranked context.\n",
    "\n",
    "### Role of LLMs in RAG\n",
    "\n",
    "After retrieving and reranking relevant chunks, the LLM:\n",
    "1. **Receives the query** and retrieved context\n",
    "2. **Generates a response** that synthesizes information from multiple sources\n",
    "3. **Maintains coherence** while staying grounded in the retrieved facts\n",
    "\n",
    "### OpenAI GPT-4o-mini\n",
    "\n",
    "We'll use OpenAI's `gpt-4o-mini` model, which provides:\n",
    "- **Fast inference** with low latency for real-time applications\n",
    "- **Cost-effective** pricing compared to larger models\n",
    "- **High quality** responses suitable for most RAG use cases\n",
    "- **Function calling** support for advanced workflows\n",
    "\n",
    "**Note**: This uses the same `OPENAI_API_KEY` environment variable as the embedder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "create-llm",
    "description": "Register OpenAI GPT-4o-mini LLM",
    "notebook": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully registered OpenAI GPT-4o-mini LLM!\n",
      "   Display Name: OpenAI GPT-4o Mini\n",
      "   LLM ID: 019b3297-d2c8-772e-9c37-f3815d3c0097\n",
      "   Provider: LLMProviderType.OPENAI\n",
      "   Model: gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "from goodmem_client.api import LLMsApi\n",
    "from goodmem_client.models import LLMCreationRequest, LLMCapabilities, ApiKeyAuth, EndpointAuthentication\n",
    "\n",
    "def create_openai_llm():\n",
    "    \"\"\"Register OpenAI GPT-4o-mini LLM with GoodMem.\"\"\"\n",
    "    \n",
    "    # Check if OPENAI_API_KEY is set\n",
    "    openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "    if not openai_api_key:\n",
    "        print(\"‚ùå OPENAI_API_KEY environment variable not set!\")\n",
    "        return None\n",
    "    \n",
    "    # Create LLMsApi instance\n",
    "    llms_api = LLMsApi(api_client=api_client)\n",
    "    \n",
    "    # Create ApiKeyAuth for OpenAI\n",
    "    api_key_auth = ApiKeyAuth(\n",
    "        inline_secret=openai_api_key,\n",
    "        header_name=\"Authorization\",\n",
    "        prefix=\"Bearer \"\n",
    "    )\n",
    "    \n",
    "    # Wrap in EndpointAuthentication\n",
    "    credentials = EndpointAuthentication(\n",
    "        kind=\"CREDENTIAL_KIND_API_KEY\",\n",
    "        api_key=api_key_auth\n",
    "    )\n",
    "    \n",
    "    # Define LLM capabilities\n",
    "    capabilities = LLMCapabilities(\n",
    "        supports_chat=True,\n",
    "        supports_completion=False,\n",
    "        supports_function_calling=True,\n",
    "        supports_system_messages=True,\n",
    "        supports_streaming=True,\n",
    "        supports_sampling_parameters=True\n",
    "    )\n",
    "    \n",
    "    # Create LLM request\n",
    "    llm_request = LLMCreationRequest(\n",
    "        display_name=\"OpenAI GPT-4o Mini\",\n",
    "        provider_type=\"OPENAI\",\n",
    "        endpoint_url=\"https://api.openai.com/v1\",\n",
    "        model_identifier=\"gpt-4o-mini\",\n",
    "        api_path=\"/chat/completions\",\n",
    "        supported_modalities=[\"TEXT\"],\n",
    "        credentials=credentials,\n",
    "        capabilities=capabilities,\n",
    "        description=\"OpenAI's GPT-4o Mini model for fast and efficient text generation\"\n",
    "    )\n",
    "    \n",
    "    # Register the LLM\n",
    "    response = llms_api.create_llm(llm_request)\n",
    "    \n",
    "    # The response has an 'llm' attribute which contains the LLMResponse\n",
    "    new_llm = response.llm    \n",
    "    return new_llm\n",
    "\n",
    "# Register or retrieve the OpenAI LLM\n",
    "openai_llm = create_openai_llm()\n",
    "print(f\"‚úÖ Successfully registered OpenAI GPT-4o-mini LLM!\")\n",
    "print(f\"   Display Name: {openai_llm.display_name}\")\n",
    "print(f\"   LLM ID: {openai_llm.llm_id}\")\n",
    "print(f\"   Provider: {openai_llm.provider_type}\")\n",
    "print(f\"   Model: {openai_llm.model_identifier}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "rag-header",
    "description": "Enhanced RAG pipeline explanation",
    "language_dependent": false,
    "notebook": "python"
   },
   "source": [
    "## Enhanced RAG with Reranking and LLM Generation\n",
    "\n",
    "Now that we have all the components configured (embedder, reranker, and LLM), let's use the complete RAG pipeline! This demonstrates the full power of GoodMem:\n",
    "\n",
    "1. **Retrieval**: Fast semantic search finds relevant chunks\n",
    "2. **Reranking**: Voyage AI reranker\n",
    " re-scores results for better relevance  \n",
    "3. **Generation**: OpenAI GPT-4o-mini generates a coherent response using the reranked context\n",
    "\n",
    "This provides significantly better answer quality compared to simple retrieval alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cell_id": "rag-pipeline",
    "description": "Complete RAG pipeline with reranking and LLM",
    "notebook": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç RAG Query: 'What is the vacation policy for employees?'\n",
      "üìÅ Space ID: 019b3293-49c8-762d-a4df-799c77c0a5d4\n",
      "üìä Max results: 3\n",
      "======================================================================\n",
      "   üìÑ Chunk 1:\n",
      "      Relevance: 0.863\n",
      "      Text: TIME OFF POLICY\n",
      "All full-time employees receive:\n",
      "- 15 days of paid vacation annually (increases to 20 days after 3 years)\n",
      "- 10 sick days per year\n",
      "- 8 ...\n",
      "\n",
      "   üìÑ Chunk 2:\n",
      "      Relevance: 0.824\n",
      "      Text: [ORGANIZATION] has established the following vacation plan to provide eligible employees \n",
      "time off with pay so that they may be free from their regula...\n",
      "\n",
      "   üìÑ Chunk 3:\n",
      "      Relevance: 0.777\n",
      "      Text: employees can use paid vacation time in minimum increments of one day.xii \n",
      "\n",
      " \n",
      "Accumulating Vacation: Employees are encouraged to use available paid va...\n",
      "\n",
      "\n",
      "ü§ñ LLM Generated Response:\n",
      "   The vacation policy for employees includes 15 days of paid vacation annually, which increases to 20 days after three years of employment. Additionally, employees receive 10 sick days per year, 8 company holidays, and personal days as needed with manager approval. Paid vacation time can be used in minimum increments of one day, and employees are encouraged to utilize their accrued vacation for rest and relaxation.\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "‚úÖ RAG completed: 8 events processed\n",
      "   LLM response: ‚úì\n",
      "   Reranked chunks: 3\n"
     ]
    }
   ],
   "source": [
    "def semantic_search_with_rag(\n",
    "        query: str, \n",
    "        space_id: str, \n",
    "        first_stage_size: int = 5,\n",
    "        max_results: int = 3, \n",
    "        reranker_id: Optional[str] = None, \n",
    "        llm_id: Optional[str] = None, \n",
    "        verbose: bool = False) -> Dict:\n",
    "    \"\"\"\n",
    "    Perform semantic search with reranking and LLM generation.\n",
    "    \n",
    "    This demonstrates the complete RAG pipeline:\n",
    "    1. Retrieval - Find relevant chunks using semantic search\n",
    "    2. Reranking - Re-score results with Voyage AI reranker\n",
    "    3. Generation - Generate answer with OpenAI GPT-4o-mini\n",
    "    \n",
    "    Args:\n",
    "        query: The search query\n",
    "        space_id: ID of the space to search\n",
    "        first_stage_size: The number of results to retrieve before postprocessing\n",
    "        max_results: Maximum number of results to return\n",
    "    \n",
    "    Returns:\n",
    "        Dict containing the LLM response and reranked chunks\n",
    "    \"\"\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"üîç RAG Query: '{query}'\")\n",
    "        print(f\"üìÅ Space ID: {space_id}\")\n",
    "        print(f\"üìä Max results: {max_results}\")\n",
    "        print(\"=\" * 70)\n",
    "    \n",
    "    event_count = 0\n",
    "    llm_response = None\n",
    "    reranked_chunks = []\n",
    "    \n",
    "    # Use retrieve_memory_stream with post-processor for RAG\n",
    "    for event in stream_client.retrieve_memory_stream(\n",
    "        message=query,\n",
    "        space_ids=[space_id],\n",
    "        requested_size=first_stage_size,\n",
    "        fetch_memory=True,\n",
    "        fetch_memory_content=False,\n",
    "        post_processor_name=\"com.goodmem.retrieval.postprocess.ChatPostProcessorFactory\",\n",
    "        post_processor_config={\n",
    "            \"llm_id\": llm_id,\n",
    "            \"reranker_id\": reranker_id,\n",
    "            \"relevance_threshold\": 0.3,\n",
    "            \"max_results\": max_results\n",
    "        },\n",
    "        format=\"ndjson\"\n",
    "    ):\n",
    "        event_count += 1\n",
    "        \n",
    "        # Handle LLM-generated response\n",
    "        if event.abstract_reply and not llm_response:\n",
    "            llm_response = event.abstract_reply.text\n",
    "            if verbose:\n",
    "                print(f\"\\nü§ñ LLM Generated Response:\")\n",
    "                print(f\"   {llm_response}\")\n",
    "                print()\n",
    "                print(\"-\" * 70)\n",
    "        \n",
    "        # Handle reranked chunks\n",
    "        if event.retrieved_item and event.retrieved_item.chunk:\n",
    "            chunk_info = event.retrieved_item.chunk\n",
    "            chunk_data = chunk_info.chunk\n",
    "            \n",
    "            reranked_chunks.append({\n",
    "                'chunk_text': chunk_data.get('chunkText', ''),\n",
    "                'relevance_score': chunk_info.relevance_score\n",
    "            })\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"   üìÑ Chunk {len(reranked_chunks)}:\")\n",
    "                print(f\"      Relevance: {chunk_info.relevance_score:.3f}\")\n",
    "                print(f\"      Text: {chunk_data.get('chunkText', '')[:150]}...\")\n",
    "                print()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"‚úÖ RAG completed: {event_count} events processed\")\n",
    "        print(f\"   LLM response: {'‚úì' if llm_response else '‚úó'}\")\n",
    "        print(f\"   Reranked chunks: {len(reranked_chunks)}\")\n",
    "    \n",
    "    return {\n",
    "        'llm_response': llm_response,\n",
    "        'chunks': reranked_chunks\n",
    "    }\n",
    "\n",
    "# Test the complete RAG pipeline\n",
    "test_query = \"What is the vacation policy for employees?\"\n",
    "rag_result = semantic_search_with_rag(\n",
    "    test_query, \n",
    "    demo_space.space_id, \n",
    "    first_stage_size=10,\n",
    "    max_results=3,\n",
    "    reranker_id=voyage_reranker.reranker_id, \n",
    "    llm_id=openai_llm.llm_id,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "rag-parameters-explanation",
    "description": "Explanation of first_stage_size and max_results",
    "notebook": "python"
   },
   "source": [
    "### Understanding Two-Stage Retrieval\n",
    "When using the complete RAG pipeline with reranking, two parameters control the retrieval process:\n",
    "\n",
    "**`first_stage_size` or `firstStageSize` or `requestedSize`** (Initial Retrieval)\n",
    "- Number of chunks retrieved from semantic search (before reranking)\n",
    "- Default: 20-50 chunks\n",
    "- Higher values ‚Üí Better recall, but slower and more expensive reranking\n",
    "- Think of it as casting a wide net\n",
    "\n",
    "**`max_results` or `maxResults`** (Final Results)\n",
    "- Number of top chunks to return after reranking\n",
    "- Default: 3-5 chunks\n",
    "- These chunks are sent to the LLM as context\n",
    "- Think of it as keeping only the best catches\n",
    "\n",
    "**Two-Stage Pipeline**:\n",
    "```\n",
    "Query ‚Üí Semantic Search ‚Üí Reranker ‚Üí Top Results ‚Üí LLM\n",
    "```\n",
    "\n",
    "**Best Practices**:\n",
    "- Increase first stage size if missing relevant content\n",
    "- Adjust max results based on LLM context window and cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "agent-integration-intro",
    "description": "Introduction to LangChain integration with GoodMem",
    "language_dependent": false,
    "notebook": "python"
   },
   "source": [
    "## LangChain / LlamaIndex Integration (Python)\n",
    "\n",
    "### Building Agentic RAG\n",
    "\n",
    "Now that we have a complete RAG pipeline, let's integrate it with **LangChain / LlamaIndex** to build an **agentic RAG system**. This allows an LLM agent to:\n",
    "\n",
    "- **Decide when to search**: The agent determines if retrieval is needed\n",
    "- **Use tools autonomously**: RAG becomes a tool the agent can call\n",
    "- **Handle complex queries**: Multi-step reasoning with retrieval\n",
    "- **Chain operations**: Combine retrieval with other capabilities\n",
    "\n",
    "### What We'll Build\n",
    "\n",
    "1. Wrap our `semantic_search_with_rag` function as a tool\n",
    "2. Create an agent with access to the retrieval tool\n",
    "3. Demonstrate the agent using retrieval to answer queries\n",
    "4. Show multi-step reasoning with streaming responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "langchain-section-header",
    "description": "LangChain integration section header",
    "hidden": true,
    "language_dependent": false,
    "notebook": "LangChain"
   },
   "source": [
    "**LangChain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "install-framework",
    "description": "Install LangChain packages",
    "language_dependent": false,
    "notebook": "LangChain"
   },
   "outputs": [],
   "source": [
    "# Install LangChain and required packages\n",
    "!pip install langchain langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "transition-to-tool-definition",
    "description": "Transition from package installation to tool definition",
    "language_dependent": false,
    "notebook": "LangChain"
   },
   "source": [
    "Now let's wrap our RAG functionality as a tool so an agent can use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "create-retrieval-tool",
    "description": "Create LangChain tool wrapper for GoodMem RAG",
    "language_dependent": false,
    "notebook": "LangChain"
   },
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve_context(query: str):\n",
    "    \"\"\"Retrieve information from the company knowledge base to help answer a query.\n",
    "    \n",
    "    This tool searches through company documents including:\n",
    "    - Employee handbooks and policies\n",
    "    - Product documentation and FAQs\n",
    "    - Security policies and procedures\n",
    "    - Technical documentation\n",
    "    \n",
    "    Use this tool when you need specific information about company policies,\n",
    "    products, or procedures to answer user questions accurately.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query or question to find relevant information for\n",
    "    \n",
    "    Returns:\n",
    "        A formatted string containing relevant context from the knowledge base,\n",
    "        along with the raw retrieved chunks as artifact data.\n",
    "    \"\"\"\n",
    "    # Call our RAG pipeline\n",
    "    result = semantic_search_with_rag(\n",
    "        query=query,\n",
    "        space_id=demo_space.space_id,\n",
    "        first_stage_size=10,\n",
    "        max_results=3,\n",
    "        reranker_id=voyage_reranker.reranker_id,\n",
    "        llm_id=None,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    serialized = \"\\n\\n\".join(\n",
    "        f\"Source Chunk {i+1} (Relevance: {chunk['relevance_score']:.3f}):\\n{chunk['chunk_text']}\"\n",
    "        for i, chunk in enumerate(result['chunks'])\n",
    "    )\n",
    "    return serialized, result['chunks']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "transition-to-agent-creation",
    "description": "Transition from tool definition to agent creation",
    "language_dependent": false,
    "notebook": "LangChain"
   },
   "source": [
    "Now let's create a agent that can use this retrieval tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cell_id": "create-agent",
    "description": "Create LangChain agent with retrieval tool",
    "language_dependent": false,
    "notebook": "LangChain"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "# Create the LLM for the agent (using the same OpenAI model)\n",
    "model = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "\n",
    "# Define the tools available to the agent\n",
    "tools = [retrieve_context]\n",
    "\n",
    "# Create custom instructions for the agent\n",
    "system_prompt = (\n",
    "    \"You are a helpful assistant with access to a company knowledge base. \"\n",
    "    \"Use the retrieve_context tool to search for information when answering \"\n",
    "    \"questions about company policies, products, procedures, or technical details. \"\n",
    "    \"Always cite the source information when using retrieved context.\"\n",
    ")\n",
    "\n",
    "# Create the agent using LangChain's create_agent\n",
    "agent = create_agent(model, tools, system_prompt=system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "transition-to-agent-test",
    "description": "Transition from agent creation to testing",
    "language_dependent": false,
    "notebook": "LangChain"
   },
   "source": [
    "Let's test the agent with a query to see it use the retrieval tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cell_id": "demo-agent",
    "description": "Demonstrate LangChain agent with streaming",
    "language_dependent": false,
    "notebook": "LangChain"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What are the security requirements for remote work?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve_context (call_FIElmZbLtnUsMKdGSFfKineN)\n",
      " Call ID: call_FIElmZbLtnUsMKdGSFfKineN\n",
      "  Args:\n",
      "    query: security requirements for remote work\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve_context\n",
      "\n",
      "Source Chunk 1 (Relevance: 0.867):\n",
      "- Report suspicious emails or security incidents immediately\n",
      "\n",
      "REMOTE WORK SECURITY\n",
      "Remote employees must follow additional security measures:\n",
      "- Use company-approved VPN for all work connections\n",
      "- Ensure home WiFi networks use WPA3 encryption\n",
      "\n",
      "Source Chunk 2 (Relevance: 0.672):\n",
      "- Keep work devices physically secure and locked when unattended\n",
      "- Use only approved cloud storage services for company data\n",
      "- Install automatic security updates on all devices\n",
      "\n",
      "INCIDENT RESPONSE\n",
      "Security incidents must be reported immediately:\n",
      "\n",
      "Source Chunk 3 (Relevance: 0.586):\n",
      "- No reuse of last 12 passwords\n",
      "- Must be changed every 90 days for privileged accounts\n",
      "- Multi-factor authentication required for all business systems\n",
      "- Password managers recommended for personal password storage\n",
      "\n",
      "ACCEPTABLE USE POLICY\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The security requirements for remote work include the following measures:\n",
      "- Use company-approved VPN for all work connections.\n",
      "- Ensure home WiFi networks use WPA3 encryption.\n",
      "- Keep work devices physically secure and locked when unattended.\n",
      "- Use only approved cloud storage services for company data.\n",
      "- Install automatic security updates on all devices.\n",
      "- Report suspicious emails or security incidents immediately.\n",
      "\n",
      "These steps help ensure the security of company data and systems while working remotely. (Source: Remote Work Security policy)\n",
      "\n",
      "======================================================================\n",
      "‚úÖ Agent completed\n"
     ]
    }
   ],
   "source": [
    "# Test the agent with a simple query\n",
    "query = \"What are the security requirements for remote work?\"\n",
    "# Stream the agent's response\n",
    "for event in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ Agent completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "llamaindex-section-header",
    "description": "LlamaIndex integration section header",
    "hidden": true,
    "language_dependent": false,
    "notebook": "LlamaIndex"
   },
   "source": [
    "**LlamaIndex**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "install-framework",
    "description": "Install LlamaIndex packages",
    "language_dependent": false,
    "notebook": "LlamaIndex"
   },
   "outputs": [],
   "source": [
    "# Install LlamaIndex and required packages\n",
    "!pip install llama-index llama-index-llms-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "transition-to-tool-definition",
    "description": "Transition from package installation to tool definition",
    "language_dependent": false,
    "notebook": "LlamaIndex"
   },
   "source": [
    "Now let's wrap our RAG functionality as a tool so an agent can use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "create-retrieval-tool",
    "description": "Create LlamaIndex tool function for GoodMem RAG",
    "language_dependent": false,
    "notebook": "LlamaIndex"
   },
   "outputs": [],
   "source": [
    "def retrieve_company_knowledge(query: str) -> str:\n",
    "    \"\"\"Retrieve information from the company knowledge base.\n",
    "    \n",
    "    This tool searches through company documents including employee handbooks,\n",
    "    product documentation, security policies, and technical documentation.\n",
    "    Use this when you need specific information about company policies,\n",
    "    products, procedures, or technical details.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query or question to find relevant information for\n",
    "    \n",
    "    Returns:\n",
    "        Formatted string containing relevant context and an AI-generated answer\n",
    "    \"\"\"\n",
    "    # Call our RAG pipeline\n",
    "    result = semantic_search_with_rag(\n",
    "        query=query,\n",
    "        space_id=demo_space.space_id,\n",
    "        first_stage_size=10,\n",
    "        max_results=3,\n",
    "        reranker_id=voyage_reranker.reranker_id,\n",
    "        llm_id=None,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    response_parts = []       \n",
    "    # Add source chunks\n",
    "    response_parts.append(\"\\nSource context:\")\n",
    "    for i, chunk in enumerate(result['chunks']):\n",
    "        response_parts.append(\n",
    "            f\"\\nChunk {i+1} (Relevance: {chunk['relevance_score']:.3f}):\\n{chunk['chunk_text'][:200]}...\"\n",
    "        )\n",
    "    return \"\\n\".join(response_parts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "transition-to-agent-creation",
    "description": "Transition from tool definition to agent creation",
    "language_dependent": false,
    "notebook": "LlamaIndex"
   },
   "source": [
    "Now let's create a agent that can use this retrieval tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "cell_id": "create-agent",
    "description": "Create LlamaIndex FunctionAgent with retrieval tool",
    "language_dependent": false,
    "notebook": "LlamaIndex"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import FunctionAgent\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# Create the LLM for the agent\n",
    "llm = OpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "\n",
    "# Create the agent with our retrieval tool\n",
    "llamaindex_agent = FunctionAgent(\n",
    "    tools=[retrieve_company_knowledge],\n",
    "    llm=llm,\n",
    "    system_prompt=(\n",
    "        \"You are a helpful assistant with access to a company knowledge base. \"\n",
    "        \"Use the retrieve_company_knowledge tool to search for information when answering \"\n",
    "        \"questions about company policies, products, procedures, or technical details. \"\n",
    "        \"Always provide clear, accurate answers based on the retrieved information.\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "transition-to-agent-test",
    "description": "Transition from agent creation to testing",
    "language_dependent": false,
    "notebook": "LlamaIndex"
   },
   "source": [
    "Let's test the agent with a query to see it use the retrieval tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "cell_id": "demo-agent",
    "description": "Demonstrate LlamaIndex agent with simple query",
    "language_dependent": false,
    "notebook": "LlamaIndex"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are the password requirements according to our security policy?\n",
      "======================================================================\n",
      "\n",
      "ü§ñ Agent Response:\n",
      "According to our security policy, the password requirements are as follows:\n",
      "- No reuse of the last 12 passwords.\n",
      "- Passwords must be changed every 90 days for privileged accounts.\n",
      "- Multi-factor authentication is required for all business systems.\n",
      "- Use of password managers is recommended for personal passwords.\n",
      "\n",
      "These measures are in place to ensure strong password security for our systems.\n",
      "\n",
      "======================================================================\n",
      "‚úÖ Agent completed\n"
     ]
    }
   ],
   "source": [
    "# Test the agent with a simple query\n",
    "query = \"What are the password requirements according to our security policy?\"\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Run the agent and stream the response\n",
    "response = await llamaindex_agent.run(query)\n",
    "\n",
    "# Print the response\n",
    "print(\"ü§ñ Agent Response:\")\n",
    "print(response)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ Agent completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "conclusion",
    "description": "Conclusion and next steps",
    "language_dependent": false,
    "notebook": "python"
   },
   "source": [
    "## üéâ Congratulations! What You Built\n",
    "\n",
    "You've successfully built a complete **Retrieval-Augmented Generation (RAG) system** using GoodMem! Let's recap what you accomplished.\n",
    "\n",
    "### Components You Configured\n",
    "\n",
    "| Component | Purpose | Function |\n",
    "|-----------|---------|----------|\n",
    "| **Embedder** | Convert text to vectors | Transform documents into semantic embeddings |\n",
    "| **Space** | Organize and store documents | Logical container with chunking configuration |\n",
    "| **Memories** | Store searchable content | Documents chunked and indexed for retrieval |\n",
    "| **Reranker** | Improve search precision | Re-score results for better relevance |\n",
    "| **LLM** | Generate natural language | Create coherent answers from retrieved context |\n",
    "\n",
    "### The Complete RAG Pipeline\n",
    "\n",
    "```\n",
    "üìÑ Documents\n",
    "   ‚Üì Chunking (256 chars, 25 overlap)\n",
    "   ‚Üì Embedding (convert to vectors)\n",
    "üóÑÔ∏è  Vector Storage (GoodMem Space)\n",
    "   ‚Üì \n",
    "üîç User Query\n",
    "   ‚Üì Semantic Search (retrieve top-K)\n",
    "   ‚Üì Reranking (re-score for precision)\n",
    "   ‚Üì Context Selection (most relevant chunks)\n",
    "ü§ñ LLM Generation (synthesize answer)\n",
    "   ‚Üì\n",
    "‚ú® Natural Language Answer\n",
    "```\n",
    "\n",
    "### Key Concepts You Learned\n",
    "\n",
    "1. **Embedders**: Transform text into semantic vectors for similarity search\n",
    "2. **Spaces**: Logical containers for organizing and searching documents\n",
    "3. **Chunking**: Breaking documents into optimal sizes for retrieval\n",
    "4. **Semantic Search**: Finding conceptually similar content, not just keyword matches\n",
    "5. **Reranking**: Two-stage retrieval for better precision\n",
    "6. **Streaming API**: Real-time, memory-efficient result processing\n",
    "7. **RAG Architecture**: Combining retrieval and generation for accurate, grounded responses\n",
    "\n",
    "### Performance Improvements\n",
    "\n",
    "**Basic search** (retrieval only):\n",
    "- Fast retrieval using vector similarity\n",
    "- Good recall, but may include less relevant results\n",
    "\n",
    "**Enhanced RAG** (with reranker + LLM):\n",
    "- Reranker improves precision significantly\n",
    "- LLM synthesizes information from multiple chunks\n",
    "- Better user experience with natural language answers\n",
    "- Grounded in actual document content (no hallucinations)\n",
    "\n",
    "### Next Steps & Advanced Topics\n",
    "\n",
    "**Enhance Your RAG System**:\n",
    "- **Multiple embedders**: Combine different embedders for better coverage\n",
    "- **Custom chunking**: Tune chunk size/overlap for your content type\n",
    "- **Metadata filtering**: Add filters to narrow search by document type, date, etc.\n",
    "- **Hybrid search**: Combine semantic and keyword search\n",
    "- **Context augmentation**: Include surrounding chunks for better LLM context\n",
    "\n",
    "**Production Deployment**:\n",
    "- **Monitoring**: Track query latency, relevance scores, user feedback\n",
    "- **Scaling**: Horizontal scaling for high-traffic applications\n",
    "- **Cost optimization**: Balance quality vs. API costs\n",
    "- **Caching**: Cache frequent queries for faster responses\n",
    "- **Error handling**: Robust exception handling and retry logic\n",
    "\n",
    "**Advanced Features**:\n",
    "- **Multi-space search**: Query across multiple knowledge bases\n",
    "- **Query expansion**: Transform queries for better retrieval\n",
    "- **Result aggregation**: Combine and deduplicate results\n",
    "- **Streaming generation**: Progressive LLM responses for real-time UX\n",
    "- **Fine-tuning**: Customize models for your specific domain\n",
    "\n",
    "### Resources\n",
    "\n",
    "- **Documentation**: [https://docs.goodmem.ai](https://docs.goodmem.ai)\n",
    "- **Community**: Join discussions and share your implementations\n",
    "- **Examples**: Explore more advanced use cases and patterns\n",
    "\n",
    "---\n",
    "\n",
    "**Great job!** You now have a solid foundation for building production RAG systems with GoodMem. üöÄ\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
